\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[nottoc,numbib]{tocbibind} % makes the BibTeX references section appear in the table of contents
\usepackage[inline]{enumitem}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{framed}
% eventually make left/right margins equal
\usepackage[top=0.6in,bottom=0.6in,left=1in,right=1in]{geometry}
\usepackage{dsfont}



\usepackage{todonotes}

\usepackage[hidelinks]{hyperref} %this needs to be loaded last!

\title{Assignment \#1 \\ \large COMP-486 Machine Learning}
\author{Xavier Silva}

\begin{document}
\maketitle
%\tableofcontents
% \newpage
\section*{Answers to Questions}
\begin{enumerate}
	% Question 1
	\item Compare the differences of the three categories (Explain, model examples, applications)
	\begin{enumerate}
		\item Supervised Learning
		\begin{itemize}
			\item Explanation:
			The training data for the machine learning algorithm includes its solutions for each instance.
			Based upon this labeled data, it can decide what should be done when given a new instance.
			\item Model Examples:
			Classification and regression.
			\item Applications:
			Classification tasks such as deciding whether an email is spam.
			Predicting a target value, such as price of a car given its individual features (regression task).
		\end{itemize}
		\item Unsupervised Learning
		\begin{itemize}
			\item Explanation:
			The training data is unlabeled and the system tries to learn without a teacher.
			\item Model examples:
			Clustering, visualization, anomaly detection (outliers), novelty detection (new instances different from all others), association rule learning (discovering hidden relations from data).
			\item Applications:
			Automatically detecting groups and patterns within data, creating useful plots from complicated data (e.g. dimensionality reduction), credit card fraud detection.
		\end{itemize}
		\item Reinforcement Learning
		\begin{itemize}
			\item Explanation:
			The learning system is called an \emph{agent}, it performs actions within its environment and it rewarded/penalized for its actions.
			It learns the best strategy to maximize rewards.
			\item Applications:
			AI that learns how to play games, training robots to walk.
		\end{itemize}
		\item Semi-supervised Learning
		\begin{itemize}
			\item Explanation:
			Only some instances of the training data is labeled while the rest is unlabeled.
			\item Model examples:
			A clustering model (unsupervised) to first label the data based upon patterns, then using a classification model (supervised) to predict the category of new data.
			\item Applications:
			Sorting family photos, it will recognize the patterns of who appears in which photos. The user is required to label the people the system clusters together.
		\end{itemize}
	\end{enumerate}
	
	% Question 2
	\item Use the code in chapter 1 to implement the Linear Regression model and \( k \)-nearest neighbors regression \( (k=4) \) using the data in \texttt{lifesat.csv}, then:
	\begin{enumerate}
		\item What is the life satisfaction prediction for Mexico with GDP = \$59,545.3 using Model-based learning? \\
			Using a linear regression model, Mexico has a life satisfaction of 7.79.
			
		\item What is the life satisfaction prediction for Mexico with GDP = \$59,545.3 using Instance-based learning? \\
			Using the \( k \)-nearest neighbors model with \( k=4 \), Mexico has a life satisfaction of 7.35.
			
		\item What is the life satisfaction prediction for Qatar with GDP = \$5009,545.3 using Model-based learning? \\
			Using a linear regression model, Qatar has a life satisfaction of 343.34.
			
		\item What is the life satisfaction prediction for Qatar with GDP = \$5009,545.3
		using Instance-based learning? \\
			Using the \( k \)-nearest neighbors model with \( k=4 \), Qatar has a life satisfaction of 7.35.
			
		\item What do you notice? Why? (Explain). \\
			Both of these countries have high GDPs compared to the training set.
			When using the linear regression model, Qatar has a prediction for life satisfaction of 343.34.
			This makes no sense, as with the context of the training data, one would expect the life satisfaction to be a measure on the range of 0 to 10 (or other similar values).
			A life satisfaction of 343.34 does not make sense here.
			The reason we get such a high number is that Qatar's GDP is a huge outlier of our data, and so the linear regression attempts to predict using the incorrect assumption that GDP and life satisfaction are in a linear correspondence.
			
			When using the instance-based model, Mexico and Qatar end up having the exact same life satisfaction.
			This occurs because of the fact that both Mexico and Qatar have high GDPs compared to the training data, which causes their \( k=4 \) closest neighbors to be the same.
			Since they use the same data points for their average, we get the same life satisfaction.
			
		\item According to the output (predictions, try some new inputs), which model would you adapt? Why?
		
		Of these two models, I would use the \( k \)-nearest neighbors regression model. 
		I would choose this model because the trend of the data does not seem to be fully linear when the GDP per capita is higher (around \$50,000).
		It would also give more reasonable predictions for outliers, such as in the given example of Qatar, where a linear regression gave an unreasonable answer while the \( k \)-nearest neighbors gave a more reasonable answer (even if it isn't close to the true life satisfaction of Qatar).
		Thus by using a \( k \)-nearest neighbors we can capture this nonlinear nature of the data.
	\end{enumerate}
	
	% Question 3
	\item Compare \textbf{Overfitting} and \textbf{Underfitting} the training data:
	\begin{enumerate}
		\item Definition
		\begin{itemize}
			\item Overfitting: The model performs well on the training data, but does not handle new data well. The model does not generalize well.
			\item Underfitting is the opposite of overfitting. The model does not generalize well, but for the opposite reasons of overfitting.
		\end{itemize}
		\item Reasons
		\begin{itemize}
			\item Overfitting occurs when the model is too complex relative to the amount and noisiness of the training data.
			\item Underfitting occurs because the model selected is not complex enough to understand the structure of the data.
		\end{itemize}
		\item Solutions
		\begin{itemize}
			\item Overfitting can be fixed by either simplifying the model, gathering more data, or cleaning up the already existing data.
			\item Underfitting can be fixed by either selecting a more powerful model, changing the features given to the learning algorithm, or reducing the constraints on the model.
		\end{itemize}
	\end{enumerate}
	
	% Question 4
	\item What is:
	\begin{enumerate}
		\item Test set? \\
		A \emph{test set} is a subset of the data which is not used to train the model, but rather to test the model after training is completed. 
		The test set should be as representative as possible of new data that could be put into the model. 
		The error rate on the test set is called the generalization error.
		
		\item Validation set? \\
		A \emph{validitation set} is a subset of the training data used during the process of selecting the best model.
		It is used as an intermediate training set that is eventually used to retrain the best model.
		Each time the model is trained a different validation set is picked from the training data.
		For example, we would split the training data into two parts, the data used to train the model and the validation set.
		The validation set should be as representative as possible of new data that could be put into the model. 
		
		\item Train-dev set? \\
		A \emph{train-dev set} is a further subset of the training data which is used to evaluate the model before using the validation set.
		
		\item Why would you want to use each of them? \\
		We want to use a test set because we want to be able to evaluate whether our model generalizes to new data. By using a test set we can measure how well our model generalizes, since we know what the expected values of the test data should be.
		
		We want to use a validation set because it allows us to compare the performance of different models. 
		By using a validation set, we can see which model performs best before testing the model on the test set.
		
		A train-dev set should be used when data that is representative of new data is scarce.
		It is useful because it allows us to test whether the model overfits the training data without having to use the scarce testing data.
		
		\item How would you use each of them? \\
		In order to use each of these sets, we would split up our data into a test set, validation set, train-dev set, and training data.
		The test and validation sets should be representative of new data that the model will predict.
		In order to use these sets, we first need to have a list of models that we will be testing.
		Now we train each model on the training data.
		Then we test each model using the train-dev set in order to ensure that the models do not overfit the training data.
		After we have trained the models and used the train-dev set, we use the validation set to measure the performance of our models.
		With this performance measure, we select the best model.
		Finally, we use the test set to measure performance of the best model before it is deployed for real world use.
	\end{enumerate}
\end{enumerate}
	
\end{document}
